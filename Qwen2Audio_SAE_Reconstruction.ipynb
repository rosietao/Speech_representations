{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b0d4f7",
   "metadata": {},
   "source": [
    "\n",
    "# Qwen2‑Audio Sparse Autoencoder (SAE) — End‑to‑End Analysis Notebook\n",
    "\n",
    "> **Goal**: Reconstruct the full pipeline you were running to analyze Qwen2‑Audio representations using OpenSAE‑style sparse autoencoders, and evaluate interpretability (monosemanticity, selectivity), alignment with phoneme/word labels (via MFA TextGrids), seed consistency, capacity sweeps, and intervention experiments.\n",
    "\n",
    "**Highlights**\n",
    "- Extract frame‑level **audio** representations from `Qwen2AudioForConditionalGeneration`'s `multi_modal_projector` output.\n",
    "- (Optional) Extract **text** token embeddings from the text encoder for cross‑modal comparisons.\n",
    "- Train **OpenSAE‑style** sparse autoencoders on audio (and later text) representations.\n",
    "- Evaluate: reconstruction quality, sparsity, UMAP/PCA, monosemanticity, phoneme/word correlations, seed consistency, capacity vs. loss.\n",
    "- **Interventions**: mask/boost single SAE units, pass through the generation stack, observe output differences.\n",
    "- Utilities for **MFA TextGrid** parsing & alignment.\n",
    "\n",
    "> Tip: Run this notebook on your machine or HPC environment with GPU. Adjust `DATA_ROOT`, `AUDIO_GLOB`, and `TRANS_GLOB` below to point to your dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b7df4",
   "metadata": {},
   "source": [
    "## 0. Environment & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56847da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally, uncomment to install needed packages.\n",
    "# !pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -U transformers accelerate datasets einops umap-learn matplotlib pandas numpy scipy tqdm sacrebleu textgrid librosa torchaudio scikit-learn networkx\n",
    "# !pip install -U rich loguru\n",
    "\n",
    "import os, sys, math, json, random, time, pathlib, glob, shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (6,4)\n",
    "plt.rcParams['figure.dpi'] = 140\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ---- Project paths (EDIT THESE) ----\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_ROOT    = PROJECT_ROOT / \"data\"          # Your audio + transcript + TextGrid live here\n",
    "AUDIO_GLOB   = str(DATA_ROOT / \"audio/**/*.wav\")   # change as needed\n",
    "TRANS_GLOB   = str(DATA_ROOT / \"transcripts/**/*.txt\")  # optional\n",
    "TEXTGRID_GLOB= str(DATA_ROOT / \"textgrid/**/*.TextGrid\") # optional\n",
    "\n",
    "OUT_DIR      = PROJECT_ROOT / \"outputs\"\n",
    "CKPT_DIR     = OUT_DIR / \"checkpoints\"\n",
    "FIG_DIR      = OUT_DIR / \"figs\"\n",
    "LOG_DIR      = OUT_DIR / \"logs\"\n",
    "for d in [OUT_DIR, CKPT_DIR, FIG_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 1337\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37228683",
   "metadata": {},
   "source": [
    "## 1. Load Qwen2‑Audio & Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b72447",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n",
    "\n",
    "QWEN_AUDIO_MODEL = os.environ.get(\"QWEN_AUDIO_MODEL\", \"Qwen/Qwen2-Audio-7B-Instruct\")  # example id; change as needed\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(QWEN_AUDIO_MODEL, trust_remote_code=True)\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\n",
    "    QWEN_AUDIO_MODEL,\n",
    "    torch_dtype=torch.float16 if DEVICE==\"cuda\" else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\" if DEVICE==\"cuda\" else None,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"Loaded:\", QWEN_AUDIO_MODEL)\n",
    "print(\"Has multi_modal_projector:\", hasattr(model, \"multi_modal_projector\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e47f2",
   "metadata": {},
   "source": [
    "## 2. Audio Dataset & Representation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import librosa\n",
    "\n",
    "SR = 16000\n",
    "MAX_SEC = 20.0\n",
    "\n",
    "def load_wav(path, sr=SR, max_sec=MAX_SEC):\n",
    "    wav, in_sr = librosa.load(path, sr=None, mono=True)\n",
    "    if in_sr != sr:\n",
    "        wav = librosa.resample(wav, orig_sr=in_sr, target_sr=sr)\n",
    "    if max_sec is not None:\n",
    "        wav = wav[:int(max_sec*sr)]\n",
    "    return torch.tensor(wav, dtype=torch.float32)\n",
    "\n",
    "def list_audio():\n",
    "    files = glob.glob(AUDIO_GLOB, recursive=True)\n",
    "    files = sorted(files)\n",
    "    return files\n",
    "\n",
    "def extract_audio_reps(wav_tensor):\n",
    "    \"\"\"Return a dict of frame-level and pooled representations.\"\"\"\n",
    "    inputs = processor(audios=wav_tensor.numpy(), sampling_rate=SR, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "    audio_proj = None\n",
    "    if hasattr(outputs, \"audio_hidden_states\") and outputs.audio_hidden_states is not None:\n",
    "        audio_last = outputs.audio_hidden_states[-1].detach().float().cpu()\n",
    "        if hasattr(model, \"multi_modal_projector\"):\n",
    "            with torch.no_grad():\n",
    "                audio_proj = model.multi_modal_projector(audio_last.to(DEVICE)).detach().float().cpu()\n",
    "        else:\n",
    "            audio_proj = audio_last\n",
    "    else:\n",
    "        hs = outputs.hidden_states\n",
    "        if isinstance(hs, (list, tuple)) and len(hs) > 0:\n",
    "            audio_proj = hs[0].detach().float().cpu()\n",
    "        else:\n",
    "            raise RuntimeError(\"Could not find audio hidden states; adjust extraction.\")\n",
    "\n",
    "    audio_proj = audio_proj.squeeze(0)  # [T_frames, D]\n",
    "    pooled = audio_proj.mean(dim=0)\n",
    "    return {\"audio_proj_frames\": audio_proj, \"pooled\": pooled, \"extra\": {}}\n",
    "\n",
    "audio_files = list_audio()[:3]\n",
    "print(\"Found audio files:\", len(audio_files))\n",
    "if audio_files:\n",
    "    rep = extract_audio_reps(load_wav(audio_files[0]))\n",
    "    print(\"Frame reps:\", rep[\"audio_proj_frames\"].shape, \"Pooled:\", rep[\"pooled\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2fc3e",
   "metadata": {},
   "source": [
    "### (Optional) Text Token Embeddings for Cross‑Modal SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5eec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = getattr(processor, \"tokenizer\", None)\n",
    "if tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(QWEN_AUDIO_MODEL, trust_remote_code=True)\n",
    "\n",
    "def extract_text_reps(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, \"language_model\") and hasattr(model.language_model, \"model\"):\n",
    "            out = model.language_model.model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        else:\n",
    "            out = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "    last = out.hidden_states[-1].detach().float().cpu()   # [1, T, D]\n",
    "    emb = last.squeeze(0)                                 # [T, D]\n",
    "    pooled = emb.mean(dim=0)\n",
    "    return {\"text_frames\": emb, \"pooled\": pooled}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8fed3",
   "metadata": {},
   "source": [
    "## 3. Build a Representation Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CACHE_DIR = OUT_DIR / \"rep_cache\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_audio_cache(limit=None, stride=1):\n",
    "    files = list_audio()\n",
    "    if limit is not None:\n",
    "        files = files[:limit]\n",
    "    rows = []\n",
    "    for ix, path in enumerate(tqdm(files, desc=\"Extracting audio reps\")):\n",
    "        if ix % stride != 0:\n",
    "            continue\n",
    "        wav = load_wav(path)\n",
    "        rep = extract_audio_reps(wav)\n",
    "        out_pt = CACHE_DIR / (Path(path).stem + \".pt\")\n",
    "        torch.save({\"frames\": rep[\"audio_proj_frames\"], \"pooled\": rep[\"pooled\"], \"path\": path}, out_pt)\n",
    "        rows.append({\"path\": path, \"pt\": str(out_pt), \"T\": rep[\"audio_proj_frames\"].shape[0]})\n",
    "    meta = pd.DataFrame(rows)\n",
    "    meta_path = CACHE_DIR / \"audio_meta.csv\"\n",
    "    meta.to_csv(meta_path, index=False)\n",
    "    print(\"Wrote:\", meta_path, \"n =\", len(meta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521d551",
   "metadata": {},
   "source": [
    "## 4. OpenSAE‑Style Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ab7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OpenSAE(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, topk=None, l1_coef=5e-4, pre_bias=True, unit_norm_decoder=True):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_hidden = d_hidden\n",
    "        self.topk = topk\n",
    "        self.l1_coef = l1_coef\n",
    "        self.pre_bias = nn.Parameter(torch.zeros(d_in)) if pre_bias else None\n",
    "\n",
    "        self.E = nn.Parameter(torch.empty(d_hidden, d_in).normal_(0, 0.02))\n",
    "        self.D = nn.Parameter(torch.empty(d_in, d_hidden).normal_(0, 0.02))\n",
    "        self.unit_norm_decoder = unit_norm_decoder\n",
    "        self.register_buffer(\"dead_mask\", torch.ones(d_hidden, dtype=torch.bool))\n",
    "\n",
    "    def encode(self, x):\n",
    "        if self.pre_bias is not None:\n",
    "            x = x + self.pre_bias\n",
    "        h = F.linear(x, self.E)  # [B, H]\n",
    "        h = F.relu(h)\n",
    "        if self.topk is not None:\n",
    "            k = min(self.topk, h.shape[-1])\n",
    "            topk_vals, topk_idx = torch.topk(h, k=k, dim=-1)\n",
    "            mask = torch.zeros_like(h)\n",
    "            mask.scatter_(dim=-1, index=topk_idx, src=torch.ones_like(topk_vals))\n",
    "            h = h * mask\n",
    "        return h\n",
    "\n",
    "    def decode(self, h):\n",
    "        D = self.D\n",
    "        if self.unit_norm_decoder:\n",
    "            D = F.normalize(D, dim=0)\n",
    "        x_hat = F.linear(h, D.t())\n",
    "        if self.pre_bias is not None:\n",
    "            x_hat = x_hat - self.pre_bias\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encode(x)\n",
    "        x_hat = self.decode(h)\n",
    "        return x_hat, h\n",
    "\n",
    "    def loss(self, x, x_hat, h):\n",
    "        rec = F.mse_loss(x_hat, x)\n",
    "        l1 = h.abs().sum(dim=-1).mean()\n",
    "        return rec + self.l1_coef * l1, {\"rec\": rec.detach(), \"l1\": l1.detach()}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def resample_dead_units(self, h_batch, thresh=1e-6):\n",
    "        mean_act = (h_batch.abs() > thresh).float().mean(dim=0)  # [H]\n",
    "        dead = mean_act < 1e-4\n",
    "        num_dead = int(dead.sum().item())\n",
    "        if num_dead > 0:\n",
    "            self.E.data[dead] = torch.empty_like(self.E.data[dead]).normal_(0, 0.02)\n",
    "            self.D.data[:, dead] = torch.empty_like(self.D.data[:, dead]).normal_(0, 0.02)\n",
    "        self.dead_mask = ~dead\n",
    "        return num_dead\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a794920",
   "metadata": {},
   "source": [
    "### 4.1 Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iter_rep_batches(meta_csv, batch_size=512, frame_sampling=\"uniform\", frames_per_item=4, device=DEVICE):\n",
    "    df = pd.read_csv(meta_csv)\n",
    "    frames = []\n",
    "    for _, row in df.iterrows():\n",
    "        d = torch.load(row[\"pt\"])\n",
    "        X = d[\"frames\"]  # [T, D]\n",
    "        T = X.shape[0]\n",
    "        if frame_sampling == \"uniform\":\n",
    "            idx = torch.linspace(0, T-1, steps=min(frames_per_item, T)).long()\n",
    "        else:\n",
    "            idx = torch.randint(0, T, (min(frames_per_item, T),))\n",
    "        frames.append(X[idx])  # [m, D]\n",
    "        if sum(z.shape[0] for z in frames) >= batch_size:\n",
    "            batch = torch.cat(frames, dim=0)[:batch_size]  # [B, D]\n",
    "            yield batch.to(device)\n",
    "            frames = []\n",
    "    if frames:\n",
    "        batch = torch.cat(frames, dim=0)\n",
    "        yield batch.to(device)\n",
    "\n",
    "def train_sae(meta_csv, d_in, d_hidden=8192, topk=64, l1=5e-4, lr=1e-3, steps=10000, log_every=100, save_every=1000, tag=\"audio_sae\"):\n",
    "    sae = OpenSAE(d_in=d_in, d_hidden=d_hidden, topk=topk, l1_coef=l1).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(sae.parameters(), lr=lr)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=steps)\n",
    "\n",
    "    log = []\n",
    "    giter = iter_rep_batches(meta_csv, batch_size=1024, frames_per_item=8)\n",
    "    for step in range(1, steps+1):\n",
    "        try:\n",
    "            x = next(giter)\n",
    "        except StopIteration:\n",
    "            giter = iter_rep_batches(meta_csv, batch_size=1024, frames_per_item=8)\n",
    "            x = next(giter)\n",
    "\n",
    "        x_hat, h = sae(x)\n",
    "        loss, parts = sae.loss(x, x_hat, h)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(sae.parameters(), max_norm=1.0)\n",
    "        opt.step(); sched.step()\n",
    "\n",
    "        if step % 250 == 0:\n",
    "            with torch.no_grad():\n",
    "                sae.resample_dead_units(h)\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            msg = {\n",
    "                \"step\": step, \"loss\": float(loss.item()),\n",
    "                \"rec\": float(parts[\"rec\"].item()), \"l1\": float(parts[\"l1\"].item()),\n",
    "                \"lr\": float(opt.param_groups[0][\"lr\"])\n",
    "            }\n",
    "            log.append(msg)\n",
    "            print(msg)\n",
    "\n",
    "        if step % save_every == 0 or step == steps:\n",
    "            ckpt_path = CKPT_DIR / f\"{tag}_step{step}.pt\"\n",
    "            torch.save({\"sae\": sae.state_dict(), \"cfg\": {\n",
    "                \"d_in\": d_in, \"d_hidden\": d_hidden, \"topk\": topk, \"l1\": l1\n",
    "            }}, ckpt_path)\n",
    "            print(\"Saved:\", ckpt_path)\n",
    "\n",
    "    pd.DataFrame(log).to_csv(OUT_DIR / f\"trainlog_{tag}.csv\", index=False)\n",
    "    return sae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f0c72a",
   "metadata": {},
   "source": [
    "## 5. Evaluation — Reconstruction & Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_reconstruction(sae, meta_csv, n_batches=20):\n",
    "    sae.eval()\n",
    "    recs, l1s, act_frac = [], [], []\n",
    "    with torch.no_grad():\n",
    "        it = iter_rep_batches(meta_csv, batch_size=2048, frames_per_item=16)\n",
    "        for i, x in zip(range(n_batches), it):\n",
    "            x_hat, h = sae(x)\n",
    "            recs.append(F.mse_loss(x_hat, x).item())\n",
    "            l1s.append(h.abs().sum(dim=-1).mean().item())\n",
    "            act_frac.append((h>0).float().mean().item())\n",
    "    return {\"rec_mse\": float(np.mean(recs)), \"l1_mean\": float(np.mean(l1s)), \"active_frac\": float(np.mean(act_frac))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307422fc",
   "metadata": {},
   "source": [
    "## 6. Visualization — PCA / UMAP of Sparse Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "\n",
    "def sample_codes(sae, meta_csv, max_samples=10000):\n",
    "    codes = []\n",
    "    with torch.no_grad():\n",
    "        for x in iter_rep_batches(meta_csv, batch_size=2048, frames_per_item=16):\n",
    "            _, h = sae(x)\n",
    "            codes.append(h.detach().cpu())\n",
    "            if sum(z.shape[0] for z in codes) >= max_samples:\n",
    "                break\n",
    "    H = torch.cat(codes, dim=0).numpy()\n",
    "    return H\n",
    "\n",
    "def plot_pca_umap(H, title=\"Sparse Codes\"):\n",
    "    pca2 = PCA(n_components=2).fit_transform(H)\n",
    "    reducer = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.1, metric=\"cosine\", random_state=SEED)\n",
    "    um2 = reducer.fit_transform(H)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10,4))\n",
    "    axs[0].scatter(pca2[:,0], pca2[:,1], s=2)\n",
    "    axs[0].set_title(\"PCA (2D)\")\n",
    "    axs[1].scatter(um2[:,0], um2[:,1], s=2)\n",
    "    axs[1].set_title(\"UMAP (2D)\")\n",
    "    fig.suptitle(title); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af43b6e6",
   "metadata": {},
   "source": [
    "## 7. MFA TextGrid Alignment — Phoneme/Word Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728fd5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from textgrid import TextGrid\n",
    "\n",
    "def parse_textgrid(tg_path):\n",
    "    tg = TextGrid.fromFile(tg_path)\n",
    "    tiers = {t.name.lower(): t for t in tg.tiers}\n",
    "    phone_tier = next((tiers[k] for k in tiers if \"phone\" in k), None)\n",
    "    word_tier  = next((tiers[k] for k in tiers if \"word\" in k), None)\n",
    "    phones = [(i.minTime, i.maxTime, i.mark) for i in phone_tier.intervals] if phone_tier else []\n",
    "    words  = [(i.minTime, i.maxTime, i.mark) for i in word_tier.intervals] if word_tier else []\n",
    "    return phones, words\n",
    "\n",
    "def time_to_frame_idx(start, end, sr=SR, hop=320):\n",
    "    s = int(round(start * sr / hop))\n",
    "    e = int(round(end   * sr / hop))\n",
    "    return s, e\n",
    "\n",
    "def unit_selectivity_to_phonemes(sae, rep_pt, phones, hop=320):\n",
    "    d = torch.load(rep_pt)\n",
    "    X = d[\"frames\"]             # [T, D]\n",
    "    H = sae.encode(X.to(DEVICE)).detach().cpu().numpy()  # [T, K]\n",
    "    results = []\n",
    "    for (t0, t1, ph) in phones:\n",
    "        s, e = time_to_frame_idx(t0, t1, hop=hop)\n",
    "        s = max(s, 0); e = min(e, H.shape[0])\n",
    "        if e <= s:\n",
    "            continue\n",
    "        h_seg = H[s:e]          # [len, K]\n",
    "        avg = h_seg.mean(axis=0)  # [K]\n",
    "        topk_idx = np.argpartition(-avg, 10)[:10]\n",
    "        results.append({\"phoneme\": ph, \"top_units\": topk_idx.tolist(), \"avg_top\": avg[topk_idx].tolist()})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59099d3b",
   "metadata": {},
   "source": [
    "## 8. Monosemanticity & Dictionary Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decoder_atoms(sae):\n",
    "    D = sae.D.detach().cpu()\n",
    "    if sae.unit_norm_decoder:\n",
    "        D = F.normalize(D, dim=0)\n",
    "    return D.numpy()  # [d_in, H]\n",
    "\n",
    "def unit_top_inputs(sae, meta_csv, unit_idx, n=2000, topm=30):\n",
    "    scores, frames = [], []\n",
    "    with torch.no_grad():\n",
    "        for x in iter_rep_batches(meta_csv, batch_size=2048, frames_per_item=16):\n",
    "            h = sae.encode(x)\n",
    "            s = h[:, unit_idx].detach().cpu()\n",
    "            scores.append(s); frames.append(x.detach().cpu())\n",
    "            if sum(len(z) for z in scores) >= n:\n",
    "                break\n",
    "    S = torch.cat(scores, dim=0)\n",
    "    X = torch.cat(frames, dim=0)\n",
    "    idx = torch.topk(S, k=min(topm, len(S))).indices\n",
    "    return X[idx], S[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c5bc9f",
   "metadata": {},
   "source": [
    "## 9. Seed Consistency — Feature Matching Across Training Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27332391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def cosine_sim_matrix(A, B):\n",
    "    A = torch.from_numpy(A).T\n",
    "    B = torch.from_numpy(B).T\n",
    "    A = F.normalize(A, dim=1); B = F.normalize(B, dim=1)\n",
    "    return (A @ B.T).cpu().numpy()\n",
    "\n",
    "def hungarian_match(sim):\n",
    "    row_ind, col_ind = linear_sum_assignment(-sim)\n",
    "    return row_ind, col_ind, float(sim[row_ind, col_ind].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba4c09",
   "metadata": {},
   "source": [
    "## 10. Capacity Sweep — d_hidden vs. Reconstruction / Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def capacity_sweep(meta_csv, d_in, hidden_list=(2048, 4096, 8192, 16384), steps=3000, topk_ratio=0.01):\n",
    "    results = []\n",
    "    for H in hidden_list:\n",
    "        topk = max(4, int(H * topk_ratio))\n",
    "        sae = train_sae(meta_csv, d_in=d_in, d_hidden=H, topk=topk, steps=steps, tag=f\"sweep_H{H}\")\n",
    "        stats = eval_reconstruction(sae, meta_csv, n_batches=20)\n",
    "        stats.update({\"H\": H, \"topk\": topk})\n",
    "        results.append(stats)\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(OUT_DIR / \"capacity_sweep.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "def plot_capacity_sweep(df):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df[\"H\"], df[\"rec_mse\"], marker=\"o\")\n",
    "    ax.set_xscale(\"log\"); ax.set_xlabel(\"Hidden units (log)\")\n",
    "    ax.set_ylabel(\"Reconstruction MSE\"); ax.set_title(\"Capacity vs Reconstruction\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e6999f",
   "metadata": {},
   "source": [
    "## 11. Intervention Experiments — Mask/Boost Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd8149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def intervene_units(sae, x, mask_units=None, boost_units=None, boost_val=3.0):\n",
    "    x = x.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        if sae.pre_bias is not None:\n",
    "            x_b = x + sae.pre_bias\n",
    "        else:\n",
    "            x_b = x\n",
    "        h = F.relu(F.linear(x_b, sae.E))\n",
    "        if mask_units:\n",
    "            h[:, mask_units] = 0.0\n",
    "        if boost_units:\n",
    "            h[:, boost_units] = h[:, boost_units] + boost_val\n",
    "        x_hat = sae.decode(h)\n",
    "    return x_hat.detach().cpu(), h.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237bb82f",
   "metadata": {},
   "source": [
    "## 12. (Future) Text‑Modality SAE for Cross‑Modal Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348a42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Once you build a cache of text token embeddings, reuse OpenSAE to train a text-side SAE;\n",
    "# then compare dictionary atoms and unit statistics between audio and text modalities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4755626",
   "metadata": {},
   "source": [
    "## 13. Runbook — Minimal Steps to Recreate Your Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a8741",
   "metadata": {},
   "source": [
    "\n",
    "1. **Set paths** in the _Environment_ cell (`DATA_ROOT`, `AUDIO_GLOB`, `TEXTGRID_GLOB`).  \n",
    "2. **Load model** (adjust `QWEN_AUDIO_MODEL` to your local checkpoint if needed).  \n",
    "3. **Build cache**: `build_audio_cache(limit=..., stride=...)`.  \n",
    "4. **Train SAE**: determine `d_in` from a cached `.pt`, then run `train_sae(...)`.  \n",
    "5. **Evaluate**: `eval_reconstruction`, `UMAP/PCA`, monosemanticity utilities.  \n",
    "6. **Phoneme/word alignment**: parse TextGrid, call `unit_selectivity_to_phonemes`.  \n",
    "7. **Seed consistency**: train multiple SAEs, Hungarian match decoder atoms.  \n",
    "8. **Capacity sweep**: run `capacity_sweep`, plot.  \n",
    "9. **Interventions**: `intervene_units` then, if desired, integrate with the generation stack to inspect output diffs.  \n",
    "10. **(Optional) Text SAE**: repeat with text embeddings and compare audio vs text features.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
